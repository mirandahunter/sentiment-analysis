{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (61182,) (61182,) Test:  ((15296,), (15296,))\n",
      "Train:  (30591,) (30591,) Test:  ((30591,), (30591,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "# Data Preprocessing\n",
    "path = \"train.tsv\"\n",
    "df = pd.read_csv(path, sep=\"\\t\") \n",
    "\n",
    "# remove punctuation\n",
    "df['Phrase'] = df['Phrase'].str.replace(r'[^\\w\\s]+', '')\n",
    "# remove numbers\n",
    "df['Phrase'] = df['Phrase'].str.replace(r'\\d+', '')\n",
    "# make it all lower case\n",
    "df['Phrase'] = df['Phrase'].str.lower()\n",
    "# remove non-asci characters\n",
    "df.Phrase.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "#df['Phrase'] = df['Phrase'].str.split()\n",
    "\n",
    "df['Tokenized_text'] = df['Phrase'].apply(word_tokenize) \n",
    "\n",
    "df['Sentiment']=df['Sentiment'].astype(int) #convert the star_rating column to int\n",
    "df['NNLabels'] = df['Sentiment'].div(4)\n",
    "\n",
    "df= df[df['Sentiment']!=2]\n",
    "\n",
    "#df['label']=np.where(df['Sentiment']>=4,1,0) #1-Positve,0-Negative\n",
    "# convert to NumPy Array\n",
    "#train = df['Phrase'].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(df['Phrase'], df['Sentiment'], test_size=0.2, random_state=30)\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"Test: \",(X_test.shape,Y_test.shape))\n",
    "\n",
    "X_train,X_valid,Y_train, Y_valid = train_test_split(X_train,Y_train, test_size=0.5, random_state=30)\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"Test: \",(X_valid.shape,Y_valid.shape))\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "tf_x_train = tfidfvectorizer.fit_transform(X_train)\n",
    "tf_x_valid = tfidfvectorizer.transform(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30591, 37)\n",
      "(30591, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "i=37\n",
    "\n",
    "scale = StandardScaler()\n",
    "X_train_normalized = scale.fit_transform(tf_x_train.toarray())\n",
    "X_valid_normalized = scale.fit_transform(tf_x_valid.toarray())\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=i, random_state=10, n_init=10)\n",
    "nn_X_train = kmeans.fit_transform(tf_x_train.toarray().astype(float))\n",
    "preds = kmeans.fit_transform(tf_x_valid.toarray().astype(float))\n",
    "nn_X_train_normalized = scale.fit_transform(nn_X_train)\n",
    "nn_X_valid_normalized = scale.fit_transform(preds)\n",
    "print(nn_X_valid_normalized.shape)\n",
    "print(nn_X_train_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(37, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64,5),\n",
    "      nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n",
      "Starting epoch 11\n",
      "Starting epoch 12\n",
      "Starting epoch 13\n",
      "Starting epoch 14\n",
      "Starting epoch 15\n",
      "Starting epoch 16\n",
      "Starting epoch 17\n",
      "Starting epoch 18\n",
      "Starting epoch 19\n",
      "Starting epoch 20\n",
      "Starting epoch 21\n",
      "Starting epoch 22\n",
      "Starting epoch 23\n",
      "Starting epoch 24\n",
      "Starting epoch 25\n",
      "Starting epoch 26\n",
      "Starting epoch 27\n",
      "Starting epoch 28\n",
      "Starting epoch 29\n",
      "Starting epoch 30\n",
      "Starting epoch 31\n",
      "Starting epoch 32\n",
      "Starting epoch 33\n",
      "Starting epoch 34\n",
      "Starting epoch 35\n",
      "Starting epoch 36\n",
      "Starting epoch 37\n",
      "Starting epoch 38\n",
      "Starting epoch 39\n",
      "Starting epoch 40\n",
      "Starting epoch 41\n",
      "Starting epoch 42\n",
      "Starting epoch 43\n",
      "Starting epoch 44\n",
      "Starting epoch 45\n",
      "Starting epoch 46\n",
      "Starting epoch 47\n",
      "Starting epoch 48\n",
      "Starting epoch 49\n",
      "Starting epoch 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the number of input features\n",
    "\n",
    "\n",
    "\n",
    "Y_tensor = torch.tensor(Y_train.array).long()\n",
    "X_tensor = torch.tensor(nn_X_train_normalized).float()\n",
    "\n",
    "#X_tensor = X_tensor.to(device)\n",
    "\n",
    "#Y_tensor = Y_tensor.to(device)\n",
    "\n",
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "#print(X_tensor.shape)\n",
    "# make a dataset object\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "# then use it to make a dataloader object\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "# make a scoring function\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        #print(outputs.shape)\n",
    "        #print(targets[4].value)\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "              print('Loss after mini-batch %5d: %.3f' %\n",
    "                    (i + 1, current_loss / 500))\n",
    "              current_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "#plt.scatter(trainX[:,0], trainX[:, 1], c=pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_valid_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(nn_X_valid_normalized)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 2\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_output_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m report\u001b[38;5;241m=\u001b[39mclassification_report(Y_train\u001b[38;5;241m.\u001b[39marray, test_output_labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m),output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "X_valid_tensor = torch.tensor(nn_X_valid_normalized).float()\n",
    "preds = mlp(X_valid_tensor.to(device)).detach().numpy()\n",
    "test_output_labels = np.argmax(preds,1)\n",
    "\n",
    "report=classification_report(Y_train.array, test_output_labels.astype(int),output_dict=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMBD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "path = \"clean_IMDBdataset.csv\"\n",
    "df = pd.read_csv(path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (40000,) (40000,) Test/Valid:  ((10000,), (10000,))\n",
      "Test:  (5000,) (5000,) Valid:  ((5000,), (5000,))\n"
     ]
    }
   ],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(df['Phrase'], df['Sentiment'], test_size=0.2, random_state=30)\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"Test/Valid: \",(X_test.shape,Y_test.shape))\n",
    "X_test,X_valid,Y_test, Y_valid = train_test_split(X_test,Y_test, test_size=0.5, random_state=30)\n",
    "print(\"Test: \" ,X_test.shape,Y_test.shape,\"Valid: \",(X_valid.shape,Y_valid.shape))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "tf_x_train = tfidfvectorizer.fit_transform(X_train)\n",
    "tf_x_valid = tfidfvectorizer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf_x_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1062\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     y \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m   1061\u001b[0m M, N \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39m_swap(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mcsr_todense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf_x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scale = StandardScaler()\n",
    "#X_train_normalized = scale.fit_transform(tf_x_train.toarray())\n",
    "#X_valid_normalized = scale.fit_transform(tf_x_valid.toarray())\n",
    "\n",
    "kmeans = KMeans(n_clusters=i, random_state=10, n_init=10)\n",
    "nn_X_train = kmeans.fit_transform(tf_x_train.toarray().astype(float))\n",
    "test_preds = kmeans.predict(tf_x_valid.toarray().astype(float))\n",
    "#preds = kmeans.fit_transform(X_valid_normalized.toarray().astype(float))\n",
    "#nn_X_train_normalized = scale.fit_transform(nn_X_train)\n",
    "#nn_X_valid_normalized = scale.fit_transform(preds)\n",
    "#print(nn_X_valid_normalized.shape)\n",
    "#print(nn_X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.45351473922902497,\n",
       "  'recall': 0.6458557588805167,\n",
       "  'f1-score': 0.532859680284192,\n",
       "  'support': 929},\n",
       " '1': {'precision': 0.36730769230769234,\n",
       "  'recall': 0.2089715536105033,\n",
       "  'f1-score': 0.2663877266387727,\n",
       "  'support': 914},\n",
       " 'accuracy': 0.42919153553988065,\n",
       " 'macro avg': {'precision': 0.41041121576835865,\n",
       "  'recall': 0.42741365624551,\n",
       "  'f1-score': 0.39962370346148235,\n",
       "  'support': 1843},\n",
       " 'weighted avg': {'precision': 0.4107620312061829,\n",
       "  'recall': 0.42919153553988065,\n",
       "  'f1-score': 0.40070809828098347,\n",
       "  'support': 1843}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report=classification_report(Y_valid, test_preds,output_dict=True)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-KMeans, then NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "path = \"clean_IMDBdataset.csv\"\n",
    "df = pd.read_csv(path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (40000,) (40000,) Test/Valid:  ((10000,), (10000,))\n",
      "Test:  (5000,) (5000,) Valid:  ((5000,), (5000,))\n"
     ]
    }
   ],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "X_train,X_test,Y_train, Y_test = train_test_split(df['Phrase'], df['Sentiment'], test_size=0.2, random_state=30)\n",
    "print(\"Train: \" ,X_train.shape,Y_train.shape,\"Test/Valid: \",(X_test.shape,Y_test.shape))\n",
    "X_test,X_valid,Y_test, Y_valid = train_test_split(X_test,Y_test, test_size=0.5, random_state=30)\n",
    "print(\"Test: \" ,X_test.shape,Y_test.shape,\"Valid: \",(X_valid.shape,Y_valid.shape))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "tf_x_train = tfidfvectorizer.fit_transform(X_train)\n",
    "tf_x_valid = tfidfvectorizer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      2\u001b[0m scale \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 3\u001b[0m X_train_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_x_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X_valid_normalized \u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m.\u001b[39mfit_transform(tf_x_valid\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:950\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[0;32m--> 950\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[1;32m    956\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m    957\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py:186\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_array_api.py:73\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.asarray\u001b[0;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Support copy in NumPy namespace\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "X_train_normalized = scale.fit_transform(tf_x_train.toarray())\n",
    "X_valid_normalized = scale.fit_transform(tf_x_valid.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=37\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "#scale = StandardScaler()\n",
    "#X_train_normalized = scale.fit_transform(tf_x_train.toarray())\n",
    "#X_valid_normalized = scale.fit_transform(tf_x_valid.toarray())\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=i, random_state=10, n_init=10)\n",
    "nn_X_train = kmeans.fit_transform(tf_x_train.toarray().astype(float))\n",
    "#test_preds = kmeans.predict(X_valid_normalized.astype(float))\n",
    "preds = kmeans.fit_transform(tf_x_valid.toarray().astype(float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 37)\n",
      "(40000, 37)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn_X_train_normalized = scale.fit_transform(nn_X_train)\n",
    "nn_X_valid_normalized = scale.fit_transform(preds)\n",
    "print(nn_X_valid_normalized.shape)\n",
    "print(nn_X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(37, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64,2),\n",
    "      nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.694\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 2\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 3\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 4\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 5\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 6\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 7\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 8\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 9\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n",
      "Starting epoch 10\n",
      "Loss after mini-batch   500: 0.692\n",
      "Loss after mini-batch  1000: 0.693\n",
      "Loss after mini-batch  1500: 0.693\n",
      "Loss after mini-batch  2000: 0.693\n",
      "Loss after mini-batch  2500: 0.693\n",
      "Loss after mini-batch  3000: 0.693\n",
      "Loss after mini-batch  3500: 0.693\n",
      "Loss after mini-batch  4000: 0.693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the number of input features\n",
    "\n",
    "\n",
    "\n",
    "Y_tensor = torch.tensor(Y_train.array).long()\n",
    "X_tensor = torch.tensor(nn_X_train).float()\n",
    "\n",
    "#X_tensor = X_tensor.to(device)\n",
    "\n",
    "#Y_tensor = Y_tensor.to(device)\n",
    "\n",
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "#print(X_tensor.shape)\n",
    "# make a dataset object\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, Y_tensor)\n",
    "\n",
    "# then use it to make a dataloader object\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "# make a scoring function\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "      # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    i=0\n",
    "    for inputs, targets in trainloader:\n",
    "        i+=1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "        #print(outputs.shape)\n",
    "        #print(targets[4].value)\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "              print('Loss after mini-batch %5d: %.3f' %\n",
    "                    (i + 1, current_loss / 500))\n",
    "              current_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "#plt.scatter(trainX[:,0], trainX[:, 1], c=pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2510}, '1': {'precision': 0.498, 'recall': 1.0, 'f1-score': 0.664886515353805, 'support': 2490}, 'accuracy': 0.498, 'macro avg': {'precision': 0.249, 'recall': 0.5, 'f1-score': 0.3324432576769025, 'support': 5000}, 'weighted avg': {'precision': 0.248004, 'recall': 0.498, 'f1-score': 0.3311134846461949, 'support': 5000}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "X_valid_tensor = torch.tensor(preds).float()\n",
    "preds1 = mlp(X_valid_tensor.to(device)).cpu().detach().numpy()\n",
    "test_output_labels = np.argmax(preds1,1)\n",
    "\n",
    "report=classification_report(Y_valid.array, test_output_labels.astype(int),output_dict=True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
